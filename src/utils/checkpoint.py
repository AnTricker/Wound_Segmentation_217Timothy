import os
import torch
from typing import Tuple, Optional, Union

def save_checkpoint(path: str, model: torch.nn.Module, optimizer: torch.optim.Optimizer, epoch: int, best_metrics: Optional[float] = None) -> None:
    """
    å„²å­˜æª¢æŸ¥é» (Checkpoint)ã€‚
    """
    os.makedirs(os.path.dirname(path), exist_ok=True)
    torch.save({
        "epoch": epoch,
        "model_state": model.state_dict(),
        "optimizer_state": optimizer.state_dict(),
        "best_metric": best_metrics,
    }, path)


def load_checkpoint(
    path: str, 
    model: torch.nn.Module, 
    optimizer: Optional[torch.optim.Optimizer] = None, 
    map_location: Union[str, torch.device] = "cpu"
) -> Tuple[int, Optional[float]]:
    """
    è¼‰å…¥æª¢æŸ¥é»ã€‚å›å‚³ (start_epoch, best_metric)ã€‚
    """
    print(f"[INFO] Loading checkpoint: {path}")
    ckpt = torch.load(path, map_location=map_location)
    
    # 1. è¼‰å…¥æ¨¡å‹æ¬Šé‡
    model.load_state_dict(ckpt["model_state"])
    
    # 2. è¼‰å…¥å„ªåŒ–å™¨ç‹€æ…‹
    if optimizer is not None:
        if "optimizer_state" in ckpt:
            optimizer.load_state_dict(ckpt["optimizer_state"])
        elif "optim_state" in ckpt:
            optimizer.load_state_dict(ckpt["optim_state"])
        elif "optimizer" in ckpt:
            optimizer.load_state_dict(ckpt["optimizer"])
        else:
            print("[WARNING] Optimizer state not found. Resuming with fresh optimizer.")

    epoch = ckpt.get("epoch", 0)
    best_metric = ckpt.get("best_metric", None)

    # ---------------------------------------------------------
    # ğŸ‘‡ é—œéµä¿®æ­£ï¼šå¦‚æœè®€åˆ°çš„æ˜¯ Tensorï¼Œå¼·åˆ¶è½‰æˆ float
    # ---------------------------------------------------------
    if isinstance(best_metric, torch.Tensor):
        best_metric = best_metric.item()

    return epoch, best_metric